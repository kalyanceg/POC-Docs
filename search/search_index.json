{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome to MkDocs"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"Intro/","text":"Linux Networking Fundamentals Target Audience Networking and Data Communication is one of the fundamental skills of an SRE. This course is framed to cover the fundamentals of Linux Networking and practical examples of how these concepts are used by an SRE in day to day work. This course can be treated as quick refresher by already working SREs and can be skimmed faster Pre - Reads This course requires high level knowledge of commonly used jargons in TCP/IP stack like DNS, TCP, UDP and HTTP. Since this course is a part of the fundamental series, familiarity with jargons is sufficient to start this course. This course also expects basic exposure to Linux command line tools. There will be needs to install certain utilities and run them as part of course exercise. What to expect from this training This course will try to cover what happens when somebody opens linkedin.com. Throughout the journey, the course covers how an SRE can optimize the system to improve her webstack performance and troubleshoot if there is an issue in any of the layers of networking stack. This course tries to dig through each layer of traditional TCP/IP stack and expects an SRE to have a picture beyond bird\u2019s eye view of the functioning of the Internet What is not covered under this training We are not covering concepts beyond fundamentals like HTTP/2.0, QUIC, TCP congestion control protocols, Anycast, BGP, CDN, Tunnels and Multicast. We expect post this course, one has relevant basics to have a quick grasp on such concepts Training Content Birds eye view of the course The course covers the question \u201cwhat happens when you open linkedin.com in your browser?\u201d The course follows the flow of TCP/IP stack . The course spends time on two Application layer protocols DNS and HTTP, two transport layer protocols UDP and TCP, networking layer protocol IP and Data Link Layer protocol(generic) Lab Environment Setup Dockerfile for this lecture is in ./build directory. Build an image using the docker file and spawn two containers executing /bin/bash with the image. sudo docker build -t network-demo . #On one terminal use this to spawn an interactive bash container sudo docker run -it network-demo /bin/bash #On another terminal run the command to spawn another container running bash sudo docker run -it network-demo /bin/bash #Find the IP of each container using ip command ip a #Make sure both containers' IPs are reachable by ping from each other, Example ping 172.17.0.3 Sections DNS - Domain Names - Resolvers - Dig - Root domains & Authoritative NS - Types of DNS records UDP - Multiplexing demultiplexing - Example socket program HTTP - Curl - Request methods - Response codes - 1.0 vs 1.1 - Stateless and cookies - Man in the middle- HTTPS TCP - Sequence numbers - 3 way handshake - retransmits - Flow control - Congestion control - Socket close/tearing down connection IP Routing and Data Link Layer - Routing table lookup - ARP - Link layer routing","title":"Intro"},{"location":"Intro/#linux-networking-fundamentals","text":"","title":"Linux Networking Fundamentals"},{"location":"Intro/#target-audience","text":"Networking and Data Communication is one of the fundamental skills of an SRE. This course is framed to cover the fundamentals of Linux Networking and practical examples of how these concepts are used by an SRE in day to day work. This course can be treated as quick refresher by already working SREs and can be skimmed faster","title":"Target Audience"},{"location":"Intro/#pre-reads","text":"This course requires high level knowledge of commonly used jargons in TCP/IP stack like DNS, TCP, UDP and HTTP. Since this course is a part of the fundamental series, familiarity with jargons is sufficient to start this course. This course also expects basic exposure to Linux command line tools. There will be needs to install certain utilities and run them as part of course exercise.","title":"Pre - Reads"},{"location":"Intro/#what-to-expect-from-this-training","text":"This course will try to cover what happens when somebody opens linkedin.com. Throughout the journey, the course covers how an SRE can optimize the system to improve her webstack performance and troubleshoot if there is an issue in any of the layers of networking stack. This course tries to dig through each layer of traditional TCP/IP stack and expects an SRE to have a picture beyond bird\u2019s eye view of the functioning of the Internet","title":"What to expect from this training"},{"location":"Intro/#what-is-not-covered-under-this-training","text":"We are not covering concepts beyond fundamentals like HTTP/2.0, QUIC, TCP congestion control protocols, Anycast, BGP, CDN, Tunnels and Multicast. We expect post this course, one has relevant basics to have a quick grasp on such concepts","title":"What is not covered under this training"},{"location":"Intro/#training-content","text":"","title":"Training Content"},{"location":"Intro/#birds-eye-view-of-the-course","text":"The course covers the question \u201cwhat happens when you open linkedin.com in your browser?\u201d The course follows the flow of TCP/IP stack . The course spends time on two Application layer protocols DNS and HTTP, two transport layer protocols UDP and TCP, networking layer protocol IP and Data Link Layer protocol(generic)","title":"Birds eye view of the course"},{"location":"Intro/#lab-environment-setup","text":"Dockerfile for this lecture is in ./build directory. Build an image using the docker file and spawn two containers executing /bin/bash with the image. sudo docker build -t network-demo . #On one terminal use this to spawn an interactive bash container sudo docker run -it network-demo /bin/bash #On another terminal run the command to spawn another container running bash sudo docker run -it network-demo /bin/bash #Find the IP of each container using ip command ip a #Make sure both containers' IPs are reachable by ping from each other, Example ping 172.17.0.3","title":"Lab Environment Setup"},{"location":"Intro/#sections","text":"DNS - Domain Names - Resolvers - Dig - Root domains & Authoritative NS - Types of DNS records UDP - Multiplexing demultiplexing - Example socket program HTTP - Curl - Request methods - Response codes - 1.0 vs 1.1 - Stateless and cookies - Man in the middle- HTTPS TCP - Sequence numbers - 3 way handshake - retransmits - Flow control - Congestion control - Socket close/tearing down connection IP Routing and Data Link Layer - Routing table lookup - ARP - Link layer routing","title":"Sections"},{"location":"dns/","text":"DNS Domain Names are simpler human readable names for websites. The Internet understands only IP addresses, but since memorizing incoherent numbers is difficult simple domain names are used instead. These domain names are translated into IP addresses by the DNS infrastructure. When somebody tries to open linkedin.com in the browser, the browser tries to convert linkedin.com to IP Address. This process is called DNS resolution. A simple pseudocode looks this ip, err = getIPAddress(domainName) if err: print(\u201cunknown Host Exception while trying to resolve:%s\u201d.format(domainName)) Now let\u2019s try to understand what happens inside the getIPAddress function. The browser would have a dns cache of its own where it checks if there is a mapping for the domainName if there is a matching available, then the browser uses that IP address. If there is no mapping, the browser calls gethostbyname syscall to ask the Operating system to find the IP address for a given DomainName def getIPAddress(domainName): resp, fail = lookupCache(domainName) If not fail: return resp else: resp, err = gethostbyname(domainName) if err: return null, err else: return resp Now lets understand what Operating system kernel does when the gethostbyname function is called. The Linux operating system looks at the file /etc/nsswitch.conf file which usually has a line hosts: files dns This line means the host lookup has to look up first in file (/etc/hosts) and then use DNS protocol to do the resolution if there is no match in /etc/hosts. The file /etc/hosts is of format IPAddress FQDN[,FQDN] 127.0.0.1 localhost.localdomain localhost ::1 localhost.localdomain localhost If a match exists for a domain in this file then that IP address is returned by the OS. Lets add a line to this file 127.0.0.1 test.linkedin.com And then do ping test.linkedin.com ping test.linkedin.com -n If no match exists in /etc/hosts, the OS tries to do a DNS resolution using DNS protocol. The linux system makes a DNS request to the first IP in /etc/resolv.conf. If there is no response, requests will be sent to subsequent servers in resolv.conf. These servers in resolv.conf are called DNS resolvers. The DNS resolvers are populated by DHCP when an IP address is assigned dynamically as the device connects to the network or statically configured by an administrator manually or using a config management system. Dig is a userspace DNS system which creates requests and sends to DNS resolvers and prints the response to the console. #run this command in one container's bash to capture all DNS requests sudo tcpdump -s 0 -A -i any port 53 #exec bash into the same container sudo docker exec -it 1cfec84d0b3a /bin/bash #make a dig request from the newly spawned bash dig linkedin.com The packet capture shows a request is made to 192.168.65.1:53 (this is the resolver in /etc/resolv.conf) for linkedin.com and a response is received from 192.168.65.1 with the IP address of linkedin.com 108.174.10.10 Now let's try to understand how DNS resolver tries to find the IP address of linkedin.com. DNS resolver first looks at its cache. Since many devices in the network can query for the domain name linkedin.com, there might be a hit in the cache. If it is a cache miss, it starts the DNS resolution process. The DNS server breaks linkedin.com. to ., com., linkedin.com. and starts DNS resolution from \u2018.\u2019. The . is called root domain and those IPs are known to the DNS resolver software. DNS resolver software asks root domain Nameservers who the right person would be to respond to details about \u2018com.\u2019. The authoritative nameserver of \u2018com.\u2019 will be given. Now the DNS resolution service will contact com\u2019s authoritative nameserver to give the authoritative nameserver for linkedin.com. Once linkedin.com\u2019s authoritative nameserver is known, the resolver asks Linkedin\u2019s NS to provide IP address for linkedin.com. This whole process can be visualized by running dig +trace linkedin.com dig +trace linkedin.com Here dig reaches out to one of the [a-l].root-servers.net. to find com\u2019s authoritative NS. com\u2019s authoritative NS is given by c.root-servers.net. Dig picks one of the authoritative NS and asks details about linkedin\u2019s NS. Here a.gtld-servers.net gives linkedin\u2019s NS details. Dig picks one of the nameservers from the list dns3.p09.nsone.net and finds the linkedin.com\u2019s IP address. Now we need to understand how com gets to know linkedin\u2019s NS record. NS has to be configured in the Domain registrar\u2019s page along with the IP addresses of name servers(glue record) which is then synced with com\u2019s Nameserver by the registrar. linkedin.com. 3600 IN A 108.174.10.10 This DNS response has 5 fields where the first field is the request and the last field is the response. The second field is the Time to Live which says how long the DNS response is valid in seconds. In this case this mapping of linkedin.com is valid for 1 hour. This is how the resolvers and application(browser) maintain their cache. Any request for linkedin.com beyond 1 hour will be treated as a cache miss as the mapping has expired its TTL and the whole process has to be redone. The 4th field says the type of DNS response/request. Some of the various DNS query types are A, AAAA, NS, TXT, PTR, MX and CNAME. - A record returns IPV4 address of the domain name - AAAA record returns the IPV6 address of the domain Name - NS record returns the authoritative nameserver for the domain name - CNAME records are aliases to the domain names. Some domains point to other domain names and resolving the latter domain name gives an IP which is used as an IP for the former domain name as well. Example www.linkedin.com\u2019s IP address is the same as 2-01-2c3e-005a.cdx.cedexis.net. - For the brevity we are not discussing other DNS record types, the RFC of each of these records are available here . dig A linkedin.com +short 108.174.10.10 dig AAAA linkedin.com +short 2620:109:c002::6cae:a0a dig NS linkedin.com +short dns3.p09.nsone.net. dns4.p09.nsone.net. dns2.p09.nsone.net. ns4.p43.dynect.net. ns1.p43.dynect.net. ns2.p43.dynect.net. ns3.p43.dynect.net. dns1.p09.nsone.net. dig www.linkedin.com CNAME +short 2-01-2c3e-005a.cdx.cedexis.net. Armed with these fundamentals of DNS lets see usecases where DNS is used by SREs. SRE Usecases Every company has to have their internal DNS infrastructure for intranet sites and internal services like databases and stuff. So there has to be a DNS infra maintained for those domain names by the infrastructure team. This DNS infra has to be optimized and scaled so that it doesn\u2019t become a single point of failure. Failure of the internal DNS infrastructure can cause API calls of microservices to fail and other cascading effect. DNS can also be used for discovering services. For example the hostname serviceb.internal.example.com could list instances which run serviceb internally in example.com company. Cloud providers provide options to enable DNS discovery( example ) DNS is used by cloud providers and CDN providers to scale their services. In Azure/AWS, Load Balancers are given a CNAME instead of IPAddress. They update the IPAddress of the Loadbalancers as they scale by changing the IP Address of alias domain names. This is one of the reasons why A records of such alias domains (Eg www-linkedin-com.l-0005.l-msedge.net. above) are short lived like 1 minute. DNS can also be used to make clients get IP addresses closer to their location so that their HTTP calls can be responded faster if the company has a presence geographically distributed. SRE also has to understand since there is no verification in DNS infrastructure, these responses can be spoofed. This is safeguarded by other protocols like HTTPS(dealt later). DNSSEC protects from forged or manipulated DNS responses. Stale DNS cache can be a problem. Some apps might still be using expired DNS records for their API calls. This is something SRE has to be wary of when doing maintenance. For DNS Loadbalancing and service discovery also, one has to understand TTL and the servers can be removed from the pool only after waiting till TTL post the changes are made to DNS records. If this is not done a certain portion of the traffic will fail as the server is removed before the TTL. With this we conclude DNS and move on to the transport layer protocol UDP used by DNS","title":"DNS"},{"location":"dns/#dns","text":"Domain Names are simpler human readable names for websites. The Internet understands only IP addresses, but since memorizing incoherent numbers is difficult simple domain names are used instead. These domain names are translated into IP addresses by the DNS infrastructure. When somebody tries to open linkedin.com in the browser, the browser tries to convert linkedin.com to IP Address. This process is called DNS resolution. A simple pseudocode looks this ip, err = getIPAddress(domainName) if err: print(\u201cunknown Host Exception while trying to resolve:%s\u201d.format(domainName)) Now let\u2019s try to understand what happens inside the getIPAddress function. The browser would have a dns cache of its own where it checks if there is a mapping for the domainName if there is a matching available, then the browser uses that IP address. If there is no mapping, the browser calls gethostbyname syscall to ask the Operating system to find the IP address for a given DomainName def getIPAddress(domainName): resp, fail = lookupCache(domainName) If not fail: return resp else: resp, err = gethostbyname(domainName) if err: return null, err else: return resp Now lets understand what Operating system kernel does when the gethostbyname function is called. The Linux operating system looks at the file /etc/nsswitch.conf file which usually has a line hosts: files dns This line means the host lookup has to look up first in file (/etc/hosts) and then use DNS protocol to do the resolution if there is no match in /etc/hosts. The file /etc/hosts is of format IPAddress FQDN[,FQDN] 127.0.0.1 localhost.localdomain localhost ::1 localhost.localdomain localhost If a match exists for a domain in this file then that IP address is returned by the OS. Lets add a line to this file 127.0.0.1 test.linkedin.com And then do ping test.linkedin.com ping test.linkedin.com -n If no match exists in /etc/hosts, the OS tries to do a DNS resolution using DNS protocol. The linux system makes a DNS request to the first IP in /etc/resolv.conf. If there is no response, requests will be sent to subsequent servers in resolv.conf. These servers in resolv.conf are called DNS resolvers. The DNS resolvers are populated by DHCP when an IP address is assigned dynamically as the device connects to the network or statically configured by an administrator manually or using a config management system. Dig is a userspace DNS system which creates requests and sends to DNS resolvers and prints the response to the console. #run this command in one container's bash to capture all DNS requests sudo tcpdump -s 0 -A -i any port 53 #exec bash into the same container sudo docker exec -it 1cfec84d0b3a /bin/bash #make a dig request from the newly spawned bash dig linkedin.com The packet capture shows a request is made to 192.168.65.1:53 (this is the resolver in /etc/resolv.conf) for linkedin.com and a response is received from 192.168.65.1 with the IP address of linkedin.com 108.174.10.10 Now let's try to understand how DNS resolver tries to find the IP address of linkedin.com. DNS resolver first looks at its cache. Since many devices in the network can query for the domain name linkedin.com, there might be a hit in the cache. If it is a cache miss, it starts the DNS resolution process. The DNS server breaks linkedin.com. to ., com., linkedin.com. and starts DNS resolution from \u2018.\u2019. The . is called root domain and those IPs are known to the DNS resolver software. DNS resolver software asks root domain Nameservers who the right person would be to respond to details about \u2018com.\u2019. The authoritative nameserver of \u2018com.\u2019 will be given. Now the DNS resolution service will contact com\u2019s authoritative nameserver to give the authoritative nameserver for linkedin.com. Once linkedin.com\u2019s authoritative nameserver is known, the resolver asks Linkedin\u2019s NS to provide IP address for linkedin.com. This whole process can be visualized by running dig +trace linkedin.com dig +trace linkedin.com Here dig reaches out to one of the [a-l].root-servers.net. to find com\u2019s authoritative NS. com\u2019s authoritative NS is given by c.root-servers.net. Dig picks one of the authoritative NS and asks details about linkedin\u2019s NS. Here a.gtld-servers.net gives linkedin\u2019s NS details. Dig picks one of the nameservers from the list dns3.p09.nsone.net and finds the linkedin.com\u2019s IP address. Now we need to understand how com gets to know linkedin\u2019s NS record. NS has to be configured in the Domain registrar\u2019s page along with the IP addresses of name servers(glue record) which is then synced with com\u2019s Nameserver by the registrar. linkedin.com. 3600 IN A 108.174.10.10 This DNS response has 5 fields where the first field is the request and the last field is the response. The second field is the Time to Live which says how long the DNS response is valid in seconds. In this case this mapping of linkedin.com is valid for 1 hour. This is how the resolvers and application(browser) maintain their cache. Any request for linkedin.com beyond 1 hour will be treated as a cache miss as the mapping has expired its TTL and the whole process has to be redone. The 4th field says the type of DNS response/request. Some of the various DNS query types are A, AAAA, NS, TXT, PTR, MX and CNAME. - A record returns IPV4 address of the domain name - AAAA record returns the IPV6 address of the domain Name - NS record returns the authoritative nameserver for the domain name - CNAME records are aliases to the domain names. Some domains point to other domain names and resolving the latter domain name gives an IP which is used as an IP for the former domain name as well. Example www.linkedin.com\u2019s IP address is the same as 2-01-2c3e-005a.cdx.cedexis.net. - For the brevity we are not discussing other DNS record types, the RFC of each of these records are available here . dig A linkedin.com +short 108.174.10.10 dig AAAA linkedin.com +short 2620:109:c002::6cae:a0a dig NS linkedin.com +short dns3.p09.nsone.net. dns4.p09.nsone.net. dns2.p09.nsone.net. ns4.p43.dynect.net. ns1.p43.dynect.net. ns2.p43.dynect.net. ns3.p43.dynect.net. dns1.p09.nsone.net. dig www.linkedin.com CNAME +short 2-01-2c3e-005a.cdx.cedexis.net. Armed with these fundamentals of DNS lets see usecases where DNS is used by SREs.","title":"DNS"},{"location":"dns/#sre-usecases","text":"Every company has to have their internal DNS infrastructure for intranet sites and internal services like databases and stuff. So there has to be a DNS infra maintained for those domain names by the infrastructure team. This DNS infra has to be optimized and scaled so that it doesn\u2019t become a single point of failure. Failure of the internal DNS infrastructure can cause API calls of microservices to fail and other cascading effect. DNS can also be used for discovering services. For example the hostname serviceb.internal.example.com could list instances which run serviceb internally in example.com company. Cloud providers provide options to enable DNS discovery( example ) DNS is used by cloud providers and CDN providers to scale their services. In Azure/AWS, Load Balancers are given a CNAME instead of IPAddress. They update the IPAddress of the Loadbalancers as they scale by changing the IP Address of alias domain names. This is one of the reasons why A records of such alias domains (Eg www-linkedin-com.l-0005.l-msedge.net. above) are short lived like 1 minute. DNS can also be used to make clients get IP addresses closer to their location so that their HTTP calls can be responded faster if the company has a presence geographically distributed. SRE also has to understand since there is no verification in DNS infrastructure, these responses can be spoofed. This is safeguarded by other protocols like HTTPS(dealt later). DNSSEC protects from forged or manipulated DNS responses. Stale DNS cache can be a problem. Some apps might still be using expired DNS records for their API calls. This is something SRE has to be wary of when doing maintenance. For DNS Loadbalancing and service discovery also, one has to understand TTL and the servers can be removed from the pool only after waiting till TTL post the changes are made to DNS records. If this is not done a certain portion of the traffic will fail as the server is removed before the TTL. With this we conclude DNS and move on to the transport layer protocol UDP used by DNS","title":"SRE Usecases"},{"location":"http/","text":"HTTP Till this point we have only got the IP address of linkedin.com. The HTML page of linkedin.com is served by HTTP protocol which the browser renders. Browser sends a HTTP request to the IP of the server determined above. Request has a verb GET, PUT, POST followed by a path and query parameters and lines of key value pair which gives information about the client and capabilities of the client like contents it can accept and a body (usually in POST or PUT) # Eg run the following in your container and have a look at the headers curl linkedin.com -v Here the first line GET is the verb / is the path and HTTP protocol version. Then there are key value pairs which give client capabilities and some details to the server. The server respond back with HTTP version, Status Code and Status message. Status codes 2xx means success, 3xx denotes redirection, 4xx denotes client side errors and 5xx server side errors. We will now jump in to see the difference between HTTP/1.0 and HTTP/1.1. #On the terminal type telnet www.linkedin.com 80 #Copy and paste the following with an empty new line at last in the telnet STDIN GET / HTTP/1.1 HOST:linkedin.com USER-AGENT: curl This would get server response and waits for next input as the underlying connection to www.linkedin.com can be reused for further queries. While going through TCP, we can understand the benefits of this. But in HTTP/1.0 this connection will be immediately closed after the response meaning new connection has to be opened for each query. HTTP/1.1 can have only one inflight request in an open connection but connection can be reused for multiple requests one after another. One of the benefits of HTTP/2.0 over HTTP/1.1 is we can have multiple inflight requests on the same connection. We are restricting our scope to generic HTTP and not jumping to the intricacies of each protocol version but they should be straight forward to understand post the course. HTTP is called stateless protocol . This section we will try to understand what stateless means. Say we logged in to linkedin.com, each request to linkedin.com from the client will have no context of the user and it makes no sense to prompt login from the user for each page/resource,This problem of HTTP is solved by COOKIE . A user is created a session when a user logs in. This session identifier is sent to the browser via SET-COOKIE header. The browser stores the COOKIE till the expiry set by the server and sends the cookie for each request from hereon for linkedin.com. More details of cookies are available here . Cookies are a critical piece of information like password and since HTTP is a plain text protocol, any man in the middle can capture either password or cookies and can breach the privacy of the user. Similarly as discussed during DNS a spoofed IP of linkedin.com can cause a phishing attack on users where an user can give linkedin\u2019s password to login on the malicious site. To solve both problems HTTPs came in place and HTTPs has to be mandated. HTTPS has to provide server identification and encryption of data between client and server. The server administrator has to generate a private public key pair and certificate request. This certificate request has to be signed by a certificate authority which converts the certificate request to a certificate. The server administrator has to update the certificate and private key to the webserver. The certificate has details about the server (like domain name for which it serves, expiry date), public key of the server. The private key is a secret to the server and losing the private key loses the trust the server provides. When clients connect, the client sends a HELLO. The server sends its certificate to the client. The client checks the validity of the cert by seeing if it is within its expiry time, if it is signed by a trusted authority and the hostname in the cert is the same as the server. This validation makes sure the server is the right server and there is no phishing. Once that is validated, the client negotiates a symmetrical key and cipher with the server by encrypting the negotiation with the public key of the server. Nobody else other than the server who has the private key can understand this data. Once negotiation is complete, that symmetric key and algorithm is used for further encryption which can be decrypted only by client and server from thereon as they only know the symmetric key and algorithm. The switch to symmetric algorithm from asymmetric encryption algorithm is to not strain the resources of client devices as symmetric encryption is generally less resource intensive than asymmetric. #Try the following on your terminal to see the cert details like Subject Name(domain name), Issuer details, Expiry date curl https://www.linkedin.com -v Here my system has a list of certificate authorities it trusts in this file /etc/pki/tls/certs/ca-bundle.crt. Curl validates the certificate is for www.linkedin.com by seeing the CN section of the subject part of the certificate. It also makes sure the certificate is not expired by seeing the expire date. It also validates the signature on the certificate by using the public key of issuer Digicert in /etc/pki/tls/certs/ca-bundle.crt. Once this is done using the public key of www.linkedin.com it negotiates cipher TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 with a symmetric key. Subsequent data transfer including first HTTP request uses the same cipher and symmetric key. Before jumping to SRE\u2019s usecases with HTTP, we will cover basics of TCP as well as most of the topics expect some basic TCP knowledge.","title":"HTTP"},{"location":"http/#http","text":"Till this point we have only got the IP address of linkedin.com. The HTML page of linkedin.com is served by HTTP protocol which the browser renders. Browser sends a HTTP request to the IP of the server determined above. Request has a verb GET, PUT, POST followed by a path and query parameters and lines of key value pair which gives information about the client and capabilities of the client like contents it can accept and a body (usually in POST or PUT) # Eg run the following in your container and have a look at the headers curl linkedin.com -v Here the first line GET is the verb / is the path and HTTP protocol version. Then there are key value pairs which give client capabilities and some details to the server. The server respond back with HTTP version, Status Code and Status message. Status codes 2xx means success, 3xx denotes redirection, 4xx denotes client side errors and 5xx server side errors. We will now jump in to see the difference between HTTP/1.0 and HTTP/1.1. #On the terminal type telnet www.linkedin.com 80 #Copy and paste the following with an empty new line at last in the telnet STDIN GET / HTTP/1.1 HOST:linkedin.com USER-AGENT: curl This would get server response and waits for next input as the underlying connection to www.linkedin.com can be reused for further queries. While going through TCP, we can understand the benefits of this. But in HTTP/1.0 this connection will be immediately closed after the response meaning new connection has to be opened for each query. HTTP/1.1 can have only one inflight request in an open connection but connection can be reused for multiple requests one after another. One of the benefits of HTTP/2.0 over HTTP/1.1 is we can have multiple inflight requests on the same connection. We are restricting our scope to generic HTTP and not jumping to the intricacies of each protocol version but they should be straight forward to understand post the course. HTTP is called stateless protocol . This section we will try to understand what stateless means. Say we logged in to linkedin.com, each request to linkedin.com from the client will have no context of the user and it makes no sense to prompt login from the user for each page/resource,This problem of HTTP is solved by COOKIE . A user is created a session when a user logs in. This session identifier is sent to the browser via SET-COOKIE header. The browser stores the COOKIE till the expiry set by the server and sends the cookie for each request from hereon for linkedin.com. More details of cookies are available here . Cookies are a critical piece of information like password and since HTTP is a plain text protocol, any man in the middle can capture either password or cookies and can breach the privacy of the user. Similarly as discussed during DNS a spoofed IP of linkedin.com can cause a phishing attack on users where an user can give linkedin\u2019s password to login on the malicious site. To solve both problems HTTPs came in place and HTTPs has to be mandated. HTTPS has to provide server identification and encryption of data between client and server. The server administrator has to generate a private public key pair and certificate request. This certificate request has to be signed by a certificate authority which converts the certificate request to a certificate. The server administrator has to update the certificate and private key to the webserver. The certificate has details about the server (like domain name for which it serves, expiry date), public key of the server. The private key is a secret to the server and losing the private key loses the trust the server provides. When clients connect, the client sends a HELLO. The server sends its certificate to the client. The client checks the validity of the cert by seeing if it is within its expiry time, if it is signed by a trusted authority and the hostname in the cert is the same as the server. This validation makes sure the server is the right server and there is no phishing. Once that is validated, the client negotiates a symmetrical key and cipher with the server by encrypting the negotiation with the public key of the server. Nobody else other than the server who has the private key can understand this data. Once negotiation is complete, that symmetric key and algorithm is used for further encryption which can be decrypted only by client and server from thereon as they only know the symmetric key and algorithm. The switch to symmetric algorithm from asymmetric encryption algorithm is to not strain the resources of client devices as symmetric encryption is generally less resource intensive than asymmetric. #Try the following on your terminal to see the cert details like Subject Name(domain name), Issuer details, Expiry date curl https://www.linkedin.com -v Here my system has a list of certificate authorities it trusts in this file /etc/pki/tls/certs/ca-bundle.crt. Curl validates the certificate is for www.linkedin.com by seeing the CN section of the subject part of the certificate. It also makes sure the certificate is not expired by seeing the expire date. It also validates the signature on the certificate by using the public key of issuer Digicert in /etc/pki/tls/certs/ca-bundle.crt. Once this is done using the public key of www.linkedin.com it negotiates cipher TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 with a symmetric key. Subsequent data transfer including first HTTP request uses the same cipher and symmetric key. Before jumping to SRE\u2019s usecases with HTTP, we will cover basics of TCP as well as most of the topics expect some basic TCP knowledge.","title":"HTTP"},{"location":"ipr/","text":"IP Routing and Data Link Layer We will dig how packets that leave the client reach the server and vice versa. When the packet reaches the IP layer, the transport layer populates source port, destination port. IP/Network layer populates destination IP(discovered from DNS) and then looks up the route to the destination IP on the routing table. #Linux route -n command gives the default routing table route -n Here the destination IP is bitwise AND\u2019d with the Genmask and if the answer is the destination part of the table then that gateway and interface is picked for routing. Here linkedin.com\u2019s IP 108.174.10.10 is AND\u2019d with 255.255.255.0 and the answer we get is 108.174.10.0 which doesn\u2019t match with any destination in the routing table. Then we AND destination IP with 0.0.0.0 and we get 0.0.0.0. This answer matches the default row Routing table is processed in the order of more octets of 1 set in genmask and genmask 0.0.0.0 is the default route if nothing matches. At the end of this operation Linux figured out that the packet has to be sent to next hop 172.17.0.1 via eth0. The source IP of the packet will be set as the IP of interface eth0. Now to send the packet to 172.17.0.1 linux has to figure out the MAC address of 172.17.0.1. MAC address is figured by looking at the internal arp cache which stores translation between IP address and MAC address. If there is a cache miss, Linux broadcasts ARP request within the internal network asking who has 172.17.0.1. The owner of the IP sends an ARP response which is cached by the kernel and the kernel sends the packet to the gateway by setting Source mac address as mac address of eth0 and destination mac address of 172.17.0.1 which we got just now. Similar routing lookup process is followed in each hop till the packet reaches the actual server. Transport layer and layers above it come to play only at end servers. During intermediate hops only till the IP/Network layer is involved. One weird gateway we saw in the routing table is 0.0.0.0. This gateway means no Layer3(Network layer) hop is needed to send the packet. Both source and destination are in the same network. Kernel has to figure out the mac of the destination and populate source and destination mac appropriately and send the packet out so that it reaches the destination without any Layer3 hop in the middle As we followed in other modules, lets complete this session with SRE usecases SRE Usecase Generally the routing table is populated by DHCP and playing around is not a good practice. There can be reasons where one has to play around the routing table but take that path only when it's absolutely necessary Understanding error messages better like, \u201cNo route to host\u201d error can mean mac address of the destination host is not found and it can mean the destination host is down On rare cases looking at the ARP table can help us understand if there is a IP conflict where same IP is assigned to two hosts by mistake and this is causing unexpected behavior With this we have traversed through the TCP/IP stack completely. We hope there will be a different perspective when one opens any website in the browser post the course. Post Training Exercises Setup own DNS resolver in the dev environment which acts as an authoritative DNS server for example.com and forwarder for other domains. Update resolv.conf to use the new DNS resolver running in localhost Set up a site dummy.example.com in localhost and run a webserver with a self signed certificate. Update the trusted CAs or pass self signed CA\u2019s public key as a parameter so that curl https://dummy.example.com -v works properly without self signed cert warning Update the routing table to use another host(container/VM) in the same network as a gateway for 8.8.8.8/32 and run ping 8.8.8.8. Do the packet capture on the new gateway to see L3 hop is working as expected(might need to disable icmp_redirect)","title":"IP Routing"},{"location":"ipr/#ip-routing-and-data-link-layer","text":"We will dig how packets that leave the client reach the server and vice versa. When the packet reaches the IP layer, the transport layer populates source port, destination port. IP/Network layer populates destination IP(discovered from DNS) and then looks up the route to the destination IP on the routing table. #Linux route -n command gives the default routing table route -n Here the destination IP is bitwise AND\u2019d with the Genmask and if the answer is the destination part of the table then that gateway and interface is picked for routing. Here linkedin.com\u2019s IP 108.174.10.10 is AND\u2019d with 255.255.255.0 and the answer we get is 108.174.10.0 which doesn\u2019t match with any destination in the routing table. Then we AND destination IP with 0.0.0.0 and we get 0.0.0.0. This answer matches the default row Routing table is processed in the order of more octets of 1 set in genmask and genmask 0.0.0.0 is the default route if nothing matches. At the end of this operation Linux figured out that the packet has to be sent to next hop 172.17.0.1 via eth0. The source IP of the packet will be set as the IP of interface eth0. Now to send the packet to 172.17.0.1 linux has to figure out the MAC address of 172.17.0.1. MAC address is figured by looking at the internal arp cache which stores translation between IP address and MAC address. If there is a cache miss, Linux broadcasts ARP request within the internal network asking who has 172.17.0.1. The owner of the IP sends an ARP response which is cached by the kernel and the kernel sends the packet to the gateway by setting Source mac address as mac address of eth0 and destination mac address of 172.17.0.1 which we got just now. Similar routing lookup process is followed in each hop till the packet reaches the actual server. Transport layer and layers above it come to play only at end servers. During intermediate hops only till the IP/Network layer is involved. One weird gateway we saw in the routing table is 0.0.0.0. This gateway means no Layer3(Network layer) hop is needed to send the packet. Both source and destination are in the same network. Kernel has to figure out the mac of the destination and populate source and destination mac appropriately and send the packet out so that it reaches the destination without any Layer3 hop in the middle As we followed in other modules, lets complete this session with SRE usecases","title":"IP Routing and Data Link Layer"},{"location":"ipr/#sre-usecase","text":"Generally the routing table is populated by DHCP and playing around is not a good practice. There can be reasons where one has to play around the routing table but take that path only when it's absolutely necessary Understanding error messages better like, \u201cNo route to host\u201d error can mean mac address of the destination host is not found and it can mean the destination host is down On rare cases looking at the ARP table can help us understand if there is a IP conflict where same IP is assigned to two hosts by mistake and this is causing unexpected behavior With this we have traversed through the TCP/IP stack completely. We hope there will be a different perspective when one opens any website in the browser post the course.","title":"SRE Usecase"},{"location":"ipr/#post-training-exercises","text":"Setup own DNS resolver in the dev environment which acts as an authoritative DNS server for example.com and forwarder for other domains. Update resolv.conf to use the new DNS resolver running in localhost Set up a site dummy.example.com in localhost and run a webserver with a self signed certificate. Update the trusted CAs or pass self signed CA\u2019s public key as a parameter so that curl https://dummy.example.com -v works properly without self signed cert warning Update the routing table to use another host(container/VM) in the same network as a gateway for 8.8.8.8/32 and run ping 8.8.8.8. Do the packet capture on the new gateway to see L3 hop is working as expected(might need to disable icmp_redirect)","title":"Post Training Exercises"},{"location":"tcp/","text":"TCP TCP is a transport layer protocol like UDP but it guarantees reliability, flow control and congestion control. TCP guarantees reliable delivery by using sequence numbers. A TCP connection is established by a three way handshake. In our case, the client sends a SYN packet along with the starting sequence number it plans to use, the server acknowledges the SYN packet and sends a SYN with its sequence number. Once the client acknowledges the syn packet, the connection is established. Each data transferred from here on is considered delivered reliably once acknowledgement for that sequence is received by the concerned party Thanks to wikipedia for this three way handshake illustration #To understand handshake run packet capture on one bash session tcpdump -S -i any port 80 #Run curl on one bash session curl www.linkedin.com Here client sends a syn flag shown by [S] flag with a sequence number 3538659162. The server acknowledges receipt of SYN with an ack [.] flag and a Syn flag for its sequence number[S]. The server uses the sequence number 452899268 and acknowledges the client it\u2019s expecting sequence number 3538659163 (client sequence+1). Client sends a zero length acknowledgement packet to the server(server sequence+1) and connection stands established. This is called three way handshake. The client sends a 80 bytes length packet after this and increments its sequence number by 80. Server sends a 315 byte response and closes the connection. This was the difference we were talking about between HTTP/1.1 and HTTP/1.0. In HTTP/1.1 this same connection can be reused which reduces overhead of 3 way handshake for each HTTP request. If a packet is missed between client and server, server won\u2019t send an ack for the client and client would retry sending the packet till the ACK is received. This guarantees reliability. The flow control is established by the win size field in each segment. The win size says available TCP buffer length in the kernel which can be used to buffer received segments. A size 0 means the receiver has a lot of lag to catch from its socket buffer and the sender has to pause sending packets so that receiver can cope up. This flow control protects from slow receiver and fast sender problem TCP also does congestion control which determines how many segments can be in transit without an ack. Linux provides us the ability to configure algorithms for congestion control which we are not covering here. While closing a connection, client/server calls a close syscall. Let's assume client do that. Client\u2019s kernel will send a FIN packet to the server. Server\u2019s kernel can\u2019t close the connection till the close syscall is called by the server application. Once server app calls close server also sends a FIN packet and client enters into time wait state for 2*MSS(120s) so that this socket can\u2019t be reused for that time period to prevent any TCP state corruptions due to stray stale packets. Thanks to wikipedia for this connection tearing illustration Armed with our TCP and HTTP knowledge lets see how this is used by SREs in their role SRE Usecases Scaling HTTP performance using load balancers need consistent knowledge about both TCP and HTTP. There are different kinds of load balancing like L4, L7 load balancing, Direct Server Return etc. HTTPs offloading can be done on Load balancer or directly on servers based on the performance and compliance needs. Tweaking sysctl variables for rmem and wmem like we did for UDP can improve throughput of sender and receiver. Sysctl variable tcp_max_syn_backlog and socket variable somax_conn determines how many connections for which the kernel can complete 3 way handshake before app calling accept syscall. This is much useful in single threaded applications. Once the backlog is full, new connections stay in SYN_RCVD state (when you run netstat) till the application calls accept syscall Apps can run out of file descriptors if there are too many short lived connections. Digging through tcp_reuse and tcp_recycle can help reduce time spent in the time wait state(it has its own risk). Making apps reuse a pool of connections instead of creating ad hoc connection can also help Understanding performance bottlenecks by seeing metrics and classifying whether its a problem in App or network side. Example too many sockets in Close_wait state is a problem on application whereas retransmissions can be a problem more on network or on OS stack than the application itself. Understanding the fundamentals can help us narrow down where the bottleneck is We are close to the answer what happens when we open linkedin.com in the browser. We haven't yet dealt with how packets take multiple hops from the browser and reach Linkedin's server. This we will deal in the final section IP Routing and Data Link Layer","title":"TCP"},{"location":"tcp/#tcp","text":"TCP is a transport layer protocol like UDP but it guarantees reliability, flow control and congestion control. TCP guarantees reliable delivery by using sequence numbers. A TCP connection is established by a three way handshake. In our case, the client sends a SYN packet along with the starting sequence number it plans to use, the server acknowledges the SYN packet and sends a SYN with its sequence number. Once the client acknowledges the syn packet, the connection is established. Each data transferred from here on is considered delivered reliably once acknowledgement for that sequence is received by the concerned party Thanks to wikipedia for this three way handshake illustration #To understand handshake run packet capture on one bash session tcpdump -S -i any port 80 #Run curl on one bash session curl www.linkedin.com Here client sends a syn flag shown by [S] flag with a sequence number 3538659162. The server acknowledges receipt of SYN with an ack [.] flag and a Syn flag for its sequence number[S]. The server uses the sequence number 452899268 and acknowledges the client it\u2019s expecting sequence number 3538659163 (client sequence+1). Client sends a zero length acknowledgement packet to the server(server sequence+1) and connection stands established. This is called three way handshake. The client sends a 80 bytes length packet after this and increments its sequence number by 80. Server sends a 315 byte response and closes the connection. This was the difference we were talking about between HTTP/1.1 and HTTP/1.0. In HTTP/1.1 this same connection can be reused which reduces overhead of 3 way handshake for each HTTP request. If a packet is missed between client and server, server won\u2019t send an ack for the client and client would retry sending the packet till the ACK is received. This guarantees reliability. The flow control is established by the win size field in each segment. The win size says available TCP buffer length in the kernel which can be used to buffer received segments. A size 0 means the receiver has a lot of lag to catch from its socket buffer and the sender has to pause sending packets so that receiver can cope up. This flow control protects from slow receiver and fast sender problem TCP also does congestion control which determines how many segments can be in transit without an ack. Linux provides us the ability to configure algorithms for congestion control which we are not covering here. While closing a connection, client/server calls a close syscall. Let's assume client do that. Client\u2019s kernel will send a FIN packet to the server. Server\u2019s kernel can\u2019t close the connection till the close syscall is called by the server application. Once server app calls close server also sends a FIN packet and client enters into time wait state for 2*MSS(120s) so that this socket can\u2019t be reused for that time period to prevent any TCP state corruptions due to stray stale packets. Thanks to wikipedia for this connection tearing illustration Armed with our TCP and HTTP knowledge lets see how this is used by SREs in their role","title":"TCP"},{"location":"tcp/#sre-usecases","text":"Scaling HTTP performance using load balancers need consistent knowledge about both TCP and HTTP. There are different kinds of load balancing like L4, L7 load balancing, Direct Server Return etc. HTTPs offloading can be done on Load balancer or directly on servers based on the performance and compliance needs. Tweaking sysctl variables for rmem and wmem like we did for UDP can improve throughput of sender and receiver. Sysctl variable tcp_max_syn_backlog and socket variable somax_conn determines how many connections for which the kernel can complete 3 way handshake before app calling accept syscall. This is much useful in single threaded applications. Once the backlog is full, new connections stay in SYN_RCVD state (when you run netstat) till the application calls accept syscall Apps can run out of file descriptors if there are too many short lived connections. Digging through tcp_reuse and tcp_recycle can help reduce time spent in the time wait state(it has its own risk). Making apps reuse a pool of connections instead of creating ad hoc connection can also help Understanding performance bottlenecks by seeing metrics and classifying whether its a problem in App or network side. Example too many sockets in Close_wait state is a problem on application whereas retransmissions can be a problem more on network or on OS stack than the application itself. Understanding the fundamentals can help us narrow down where the bottleneck is We are close to the answer what happens when we open linkedin.com in the browser. We haven't yet dealt with how packets take multiple hops from the browser and reach Linkedin's server. This we will deal in the final section IP Routing and Data Link Layer","title":"SRE Usecases"},{"location":"udp/","text":"UDP UDP is a transport layer protocol. DNS is an application layer protocol that runs on top of UDP(most of the times). Before jumping into UDP, let's try to understand what an application and transport layer is. DNS protocol is used by a DNS client(eg dig) and DNS server(eg named). The transport layer makes sure the DNS request reaches the DNS server process and similarly the response reaches the DNS client process. Multiple processes can run on a system and they can listen on any ports. DNS servers usually listen on port 53. When a client makes a DNS request, after filling the necessary application payload, it passes the payload to the kernel via sendto system call. The kernel picks a random port number( 1024 ) as source port number and puts 53 as destination port number and sends the packet to lower layers. When the kernel on server side receives the packet, it checks the port number and queues the packet to the application buffer of the DNS server process which makes a recvfrom system call and reads the packet. This process by the kernel is called multiplexing(combining packets from multiple applications to same lower layers) and demultiplexing(segregating packets from single lower layer to multiple applications). Multiplexing and Demultiplexing is done by the Transport layer. UDP is one of the simplest transport layer protocol and it does only multiplexing and demultiplexing. Another common transport layer protocol TCP does a bunch of other things like reliable communication, flow control and congestion control. UDP is designed to be lightweight and handle communications with little overhead. So it doesn\u2019t do anything beyond multiplexing and demultiplexing. If applications running on top of UDP need any of the features of TCP, they have to implement that in their application This example from python wiki covers a sample UDP client and server where \u201cHello World\u201d is an application payload sent to server listening on port 5005. The server receives the packet and prints the \u201cHello World\u201d string from the client SRE Usecases If the underlying network is slow and the UDP layer is unable to queue packets down to the networking layer, sendto syscall from the application will hang till the kernel finds some of its buffer is freed. This can affect the throughput of the system. Increasing write memory buffer values using sysctl variables net.core.wmem_max and net.core.wmem_default provides some cushion to the application from the slow network Similarly if the receiver process is slow in consuming from its buffer, the kernel has to drop packets which it can\u2019t queue due to the buffer being full. Since UDP doesn\u2019t guarantee reliability these dropped packets can cause data loss unless tracked by the application layer. Increasing sysctl variables rmem_default and rmem_max can provide some cushion to slow applications from fast senders. Now that we have got the IP of linkedin.com using DNS protocol, we will look at HTTP protocol which provides the HTML resources to be rendered by the browser.","title":"UDP"},{"location":"udp/#udp","text":"UDP is a transport layer protocol. DNS is an application layer protocol that runs on top of UDP(most of the times). Before jumping into UDP, let's try to understand what an application and transport layer is. DNS protocol is used by a DNS client(eg dig) and DNS server(eg named). The transport layer makes sure the DNS request reaches the DNS server process and similarly the response reaches the DNS client process. Multiple processes can run on a system and they can listen on any ports. DNS servers usually listen on port 53. When a client makes a DNS request, after filling the necessary application payload, it passes the payload to the kernel via sendto system call. The kernel picks a random port number( 1024 ) as source port number and puts 53 as destination port number and sends the packet to lower layers. When the kernel on server side receives the packet, it checks the port number and queues the packet to the application buffer of the DNS server process which makes a recvfrom system call and reads the packet. This process by the kernel is called multiplexing(combining packets from multiple applications to same lower layers) and demultiplexing(segregating packets from single lower layer to multiple applications). Multiplexing and Demultiplexing is done by the Transport layer. UDP is one of the simplest transport layer protocol and it does only multiplexing and demultiplexing. Another common transport layer protocol TCP does a bunch of other things like reliable communication, flow control and congestion control. UDP is designed to be lightweight and handle communications with little overhead. So it doesn\u2019t do anything beyond multiplexing and demultiplexing. If applications running on top of UDP need any of the features of TCP, they have to implement that in their application This example from python wiki covers a sample UDP client and server where \u201cHello World\u201d is an application payload sent to server listening on port 5005. The server receives the packet and prints the \u201cHello World\u201d string from the client","title":"UDP"},{"location":"udp/#sre-usecases","text":"If the underlying network is slow and the UDP layer is unable to queue packets down to the networking layer, sendto syscall from the application will hang till the kernel finds some of its buffer is freed. This can affect the throughput of the system. Increasing write memory buffer values using sysctl variables net.core.wmem_max and net.core.wmem_default provides some cushion to the application from the slow network Similarly if the receiver process is slow in consuming from its buffer, the kernel has to drop packets which it can\u2019t queue due to the buffer being full. Since UDP doesn\u2019t guarantee reliability these dropped packets can cause data loss unless tracked by the application layer. Increasing sysctl variables rmem_default and rmem_max can provide some cushion to slow applications from fast senders. Now that we have got the IP of linkedin.com using DNS protocol, we will look at HTTP protocol which provides the HTML resources to be rendered by the browser.","title":"SRE Usecases"}]}